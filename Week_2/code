import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split

#Runge function
def runge_function(x):
    return 1 / (1 + 25 * x**2)

#dataset and training/validation sets
x = np.linspace(-1, 1, 200).reshape(-1, 1).astype(np.float32)
y = runge_function(x).astype(np.float32)
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

x_train_tensor = torch.from_numpy(x_train)
y_train_tensor = torch.from_numpy(y_train)
x_val_tensor = torch.from_numpy(x_val)
y_val_tensor = torch.from_numpy(y_val)

#neural network
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(1, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 1)
        self.activation = nn.Tanh()

    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.fc3(x)
        return x

net = Net()

#training
criterion = nn.MSELoss()
optimizer = optim.Adam(net.parameters(), lr=0.01)

epochs = 1000
train_losses, val_losses = [], []

for epoch in range(epochs):
    #training
    optimizer.zero_grad()
    outputs = net(x_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()

    #validation
    with torch.no_grad():
        val_outputs = net(x_val_tensor)
        val_loss = criterion(val_outputs, y_val_tensor)

    #record losses
    train_losses.append(loss.item())
    val_losses.append(val_loss.item())

    if (epoch+1) % 200 == 0:
        print(f"Epoch [{epoch+1}/{epochs}] "
              f"Train Loss: {loss.item():.6f}, "
              f"Val Loss: {val_loss.item():.6f}")

#training/validation loss curves
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label="Training Loss")
plt.plot(val_losses, label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("MSE Loss")
plt.title("Training/Validation Loss Curves")
plt.legend()

#test the network and plot approximation
x_test = np.linspace(-1, 1, 500).reshape(-1, 1).astype(np.float32)
y_test = runge_function(x_test)

x_test_tensor = torch.from_numpy(x_test)
y_pred = net(x_test_tensor).detach().numpy()

plt.subplot(1, 2, 2)
plt.plot(x_test, y_test, label="Runge function", color="blue")
plt.plot(x_test, y_pred, label="Neural Net Approx", color="red", linestyle="--")
plt.legend()
plt.title("Neural Network Approximation of Runge Function")

plt.tight_layout()
plt.show()

#MSE
with torch.no_grad():
    y_pred_tensor = net(x_test_tensor)
    test_mse = criterion(y_pred_tensor, torch.from_numpy(y_test))
    print(f"Final Test MSE: {test_mse.item():.6f}")

#derivative MSE
net.eval()
x_test = np.linspace(-1, 1, 500).reshape(-1, 1).astype(np.float32)
x_test_tensor = torch.from_numpy(x_test)
x_test_tensor.requires_grad_(True) 

y_hat = net(x_test_tensor)
dy_dx_hat = torch.autograd.grad(
    outputs = y_hat,
    inputs = x_test_tensor,
    grad_outputs = torch.ones_like(y_hat),
    retain_graph = False,
    create_graph = False,
)[0]
