import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
torch.manual_seed(0)
np.random.seed(0)

#Runge function and derivative
def runge_function(x):
    return 1.0 / (1.0 + 25.0 * x**2)

def runge_derivative_np(x_np):
    # f'(x) = -50x / (1+25 x^2)^2  (numpy)
    return -(50.0 * x_np) / (1.0 + 25.0 * x_np**2)**2

#dataset
x = np.linspace(-1, 1, 200).reshape(-1, 1).astype(np.float32)
y = runge_function(x).astype(np.float32)
x_train, x_val, y_train, y_val = train_test_split(
    x, y, test_size=0.2, random_state=42
)

x_train_t = torch.from_numpy(x_train)
y_train_t = torch.from_numpy(y_train)
x_val_t = torch.from_numpy(x_val)
y_val_t = torch.from_numpy(y_val)

dy_train_t = torch.from_numpy(runge_derivative_np(x_train).astype(np.float32))
dy_val_t = torch.from_numpy(runge_derivative_np(x_val).astype(np.float32))

#model
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(1, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 1)
        self.activation = nn.Tanh()
    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.fc3(x)
        return x

net = Net()

#Loss:= function loss + derivative loss
mse = nn.MSELoss()

train_loss_all, val_loss_all = [], []
train_loss_f, train_loss_df = [], []
val_loss_f, val_loss_df = [], []

lambda_deriv = 1.0
lr = 0.01
optimizer = optim.Adam(net.parameters(), lr=lr)
epochs = 1000

for epoch in range(epochs):
    net.train()
    optimizer.zero_grad()

    x_train_req = x_train_t.detach().clone().requires_grad_(True)
    
    y_hat = net(x_train_req)
    
    dy_hat = torch.autograd.grad(
        outputs = y_hat,
        inputs = x_train_req,
        grad_outputs = torch.ones_like(y_hat),
        create_graph = True,
        retain_graph = True
    )[0]

    loss_f  = (y_hat - y_train_t).pow(2).mean()
    loss_df = (dy_hat - dy_train_t).pow(2).mean()
    loss = loss_f + lambda_deriv * loss_df

    loss.backward()
    optimizer.step()

    net.eval()
    with torch.no_grad():
        y_val_hat = net(x_val_t)
        loss_f_val = (y_val_hat - y_val_t).pow(2).mean().item()

    x_val_req = x_val_t.detach().clone().requires_grad_(True)
    y_val_hat2 = net(x_val_req)
    dy_val_hat = torch.autograd.grad(
        outputs = y_val_hat2,
        inputs = x_val_req,
        grad_outputs = torch.ones_like(y_val_hat2),
        create_graph = False,
        retain_graph = False
    )[0].detach()
    loss_df_val = (dy_val_hat - dy_val_t).pow(2).mean().item()

    train_loss_all.append(loss.item())
    val_loss_all.append(loss_f_val + lambda_deriv * loss_df_val)
    train_loss_f.append(loss_f.item())
    train_loss_df.append(loss_df.item())
    val_loss_f.append(loss_f_val)
    val_loss_df.append(loss_df_val)

    if (epoch + 1) % 200 == 0:
        print(f"Epoch [{epoch+1}/{epochs}] "
              f"Train Total: {train_loss_all[-1]:.6f} "
              f"(f: {train_loss_f[-1]:.6f}, f': {train_loss_df[-1]:.6f}) | "
              f"Val Total: {val_loss_all[-1]:.6f} "
              f"(f: {val_loss_f[-1]:.6f}, f': {val_loss_df[-1]:.6f})")

#true f(x) vs NN prediction
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(x_test, y_test, label="True f(x)")
plt.plot(x_test, net(torch.from_numpy(x_test)).detach().numpy(),
         "--", label="NN prediction")
plt.xlabel("x"); plt.ylabel("f")
plt.title("True vs NN Prediction")
plt.legend()
plt.tight_layout()
plt.show()

#training/validation total loss curves
plt.figure(figsize=(12,5))
plt.subplot(1,2,2)
plt.plot(train_loss_all, label="Train Total Loss")
plt.plot(val_loss_all, label="Val Total Loss")
plt.xlabel("Epoch"); plt.ylabel("Loss")
plt.title("Training/Validation Loss Curves")
plt.legend()
plt.tight_layout()
plt.show()

#final test MSE
criterion = nn.MSELoss()
with torch.no_grad():
    test_mse_f = criterion(torch.from_numpy(y_test), net(torch.from_numpy(x_test))).item()

x_test_req2 = torch.from_numpy(x_test).requires_grad_(True)
y_hat_t = net(x_test_req2)
dy_hat_t = torch.autograd.grad(
    outputs = y_hat_t,
    inputs = x_test_req2,
    grad_outputs = torch.ones_like(y_hat_t),
    create_graph = False,
    retain_graph = False
)[0].detach()
test_mse_df = criterion(dy_hat_t, torch.from_numpy(dy_true)).item()

print(f"Final Test MSE (f):   {test_mse_f:.6f}")
print(f"Final Test MSE (f'):  {test_mse_df:.6f}")
